import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np


class AttentionBlock(nn.Module):
    def __init__(self, in_channels_x, in_channels_g, int_channels):
        super(AttentionBlock, self).__init__()
        # print(in_channels_x, int_channels)
        self.Wx = nn.Sequential(nn.Conv2d(in_channels_x, int_channels, kernel_size=1), nn.BatchNorm2d(int_channels))
        self.Wg = nn.Sequential(nn.Conv2d(in_channels_g, int_channels, kernel_size=1), nn.BatchNorm2d(int_channels))
        self.psi = nn.Sequential(nn.Conv2d(int_channels, 1, kernel_size=1), nn.BatchNorm2d(1), nn.Sigmoid())
        # print(self.Wx)

    def forward(self, x, g):
        # apply the Wx to the skip connection
        # print(np.shape(x))
        x1 = self.Wx(x)
        # after applying Wg to the input, upsample to the size of the skip connection
        g1 = nn.functional.interpolate(self.Wg(g), x1.shape[2:], mode='bilinear', align_corners=False)
        out = self.psi(nn.ReLU()(x1 + g1))
        out = nn.Sigmoid()(out)
        return out * x
